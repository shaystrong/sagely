{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038196,
     "end_time": "2019-08-04T22:09:45.602054",
     "exception": false,
     "start_time": "2019-08-04T22:09:45.563858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this part II notebook, we will upload the data to AWS S3 that we generated for training in the previous notebook. We will kick off an AWS Sagemaker object detection job and monitor the results. At the end of this notebook, you will have trained your own OSM-based CNN object detector!\n",
    "\n",
    "![](assets/happycloud.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 1.205385,
     "end_time": "2019-08-04T22:09:46.827507",
     "exception": false,
     "start_time": "2019-08-04T22:09:45.622122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00827,
     "end_time": "2019-08-04T22:09:46.858176",
     "exception": false,
     "start_time": "2019-08-04T22:09:46.849906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use 'papermill' (https://github.com/nteract/papermill) to pass sensitive variables to this jupyter notebook. Things like passwords, cloud locations, etc, should be paramterized as a best practice -- Never stored in a repo (especially public facing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.015293,
     "end_time": "2019-08-04T22:09:46.917447",
     "exception": false,
     "start_time": "2019-08-04T22:09:46.902154",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sage_bucket=''\n",
    "my_bucket=''\n",
    "prefix = my_bucket   #this is your model prefix\n",
    "sessname =''\n",
    "nclass = 1\n",
    "epochs =2\n",
    "mini_batch_size =2\n",
    "lr = 0.001\n",
    "lr_scheduler_factor =0.1\n",
    "momentum =0.9\n",
    "weight_decay =0.0005\n",
    "overlap = 0.5\n",
    "momentum = 0.45\n",
    "weight_decay =0.0005\n",
    "nms_thresh = 0.45\n",
    "image_shape =256\n",
    "label_width =150\n",
    "n_train_samples = 16551\n",
    "network ='resnet-50'\n",
    "optim = 'sgd'\n",
    "role = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.027079,
     "end_time": "2019-08-04T22:09:46.950439",
     "exception": false,
     "start_time": "2019-08-04T22:09:46.923360",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "sage_bucket = \"sagemaker-us-east-2-771575179338\"\n",
    "my_bucket = \"geohack_sbs\"\n",
    "role = \"AmazonSageMaker-ExecutionRole-20171129T143774\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 5.445173,
     "end_time": "2019-08-04T22:09:52.401683",
     "exception": false,
     "start_time": "2019-08-04T22:09:46.956510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3.upload_file('rec/val.rec', sage_bucket, my_bucket+'/validatation/val.rec')\n",
    "s3.upload_file('rec/train.rec', sage_bucket, my_bucket+'/train/train.rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 1.212207,
     "end_time": "2019-08-04T22:09:53.623703",
     "exception": false,
     "start_time": "2019-08-04T22:09:52.411496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "#sage_bucket='sagemaker-us-east-2-771575179338'\n",
    "#my_bucket='geohack_sbs'\n",
    "training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version=\"latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 294.445411,
     "end_time": "2019-08-04T22:14:48.077465",
     "exception": false,
     "start_time": "2019-08-04T22:09:53.632054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-04 22:09:54 Starting - Starting the training job"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:09:56 Starting - Launching requested ML instances"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:11:25 Starting - Preparing the instances for training"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:12:26 Downloading - Downloading input data"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:12:47 Training - Downloading the training image"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'label_width': u'350', u'early_stopping_min_epochs': u'10', u'epochs': u'30', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'32', u'use_pretrained_model': u'0', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.9', u'num_training_samples': u'', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'', u'base_network': u'vgg-16', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'300'}\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'lr_scheduler_step': u'3,6', u'weight_decay': u'0.0005', u'mini_batch_size': u'2', u'optimizer': u'sgd', u'base_network': u'resnet-50', u'learning_rate': u'0.001', u'use_pretrained_model': u'1', u'label_width': u'150', u'epochs': u'2', u'overlap_threshold': u'0.5', u'num_training_samples': u'16551', u'num_classes': u'1', u'nms_threshold': u'0.45', u'image_shape': u'256', u'momentum': u'0.45', u'lr_scheduler_factor': u'0.1'}\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Final configuration: {u'label_width': u'150', u'early_stopping_min_epochs': u'10', u'epochs': u'2', u'overlap_threshold': u'0.5', u'lr_scheduler_factor': u'0.1', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0005', u'mini_batch_size': u'2', u'use_pretrained_model': u'1', u'freeze_layer_pattern': u'', u'lr_scheduler_step': u'3,6', u'early_stopping': u'False', u'early_stopping_patience': u'5', u'momentum': u'0.45', u'num_training_samples': u'16551', u'optimizer': u'sgd', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.0', u'learning_rate': u'0.001', u'kv_store': u'device', u'nms_threshold': u'0.45', u'num_classes': u'1', u'base_network': u'resnet-50', u'nms_topk': u'400', u'_kvstore': u'device', u'image_shape': u'256'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Using default worker.\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Loaded iterator creator application/x-image for content type ('application/x-image', '1.0')\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Loaded iterator creator application/x-recordio for content type ('application/x-recordio', '1.0')\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Loaded iterator creator image/png for content type ('image/png', '1.0')\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Loaded iterator creator image/jpeg for content type ('image/jpeg', '1.0')\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] nvidia-smi took: 0.0251929759979 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 INFO 139645234329408] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:46 WARNING 139645234329408] Training images are resized to image shape (3, 256, 256)\u001b[0m\n",
      "\u001b[31m[22:13:46] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, use 3 threads for decoding..\u001b[0m\n",
      "\u001b[31m[22:13:46] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/train/train.rec, label padding width: 150\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:47 WARNING 139645234329408] Validation images are resized to image shape (3, 256, 256)\u001b[0m\n",
      "\u001b[31m[22:13:47] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:283: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, use 3 threads for decoding..\u001b[0m\n",
      "\u001b[31m[22:13:47] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/io/iter_image_det_recordio.cc:340: ImageDetRecordIOParser: /opt/ml/input/data/validation/val.rec, label padding width: 150\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:47 INFO 139645234329408] Creating new state\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:47 INFO 139645234329408] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:47 INFO 139645234329408] Create Store: device\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:47 INFO 139645234329408] Start training with (gpu(0)) from pretrained model 1\u001b[0m\n",
      "\u001b[31m[22:13:47] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[31m[22:13:47] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:13:48 INFO 139645234329408] done loading checkpoint\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:13:44 Training - Training image download completed. Training in progress."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1564956831.838047, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1564956831.837944}\n",
      "\u001b[0m\n",
      "\u001b[31m[22:13:51] /opt/brazil-pkg-cache/packages/MXNetECL/MXNetECL-master.1426.0/AL2012/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/04/2019 22:14:02 WARNING 139645234329408] Expected number of batches: 8275, did not match the number of batches processed: 10. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:02 INFO 139645234329408] #quality_metric: host=algo-1, epoch=0, batch=10 train cross_entropy <loss>=(0.942219278454)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:02 INFO 139645234329408] #quality_metric: host=algo-1, epoch=0, batch=10 train smooth_l1 <loss>=(1.35968596534)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:02 INFO 139645234329408] Round of batches complete\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:02 INFO 139645234329408] Updated the metrics\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:03 INFO 139645234329408] #quality_metric: host=algo-1, epoch=0, validation mAP <score>=(0.00784051728693)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:03 INFO 139645234329408] Updating the best model with validation-mAP=0.00784051728693\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:03 INFO 139645234329408] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:03 INFO 139645234329408] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1564956843.295843, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 0}, \"StartTime\": 1564956831.838505}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 WARNING 139645234329408] Expected number of batches: 8275, did not match the number of batches processed: 9. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] #quality_metric: host=algo-1, epoch=1, batch=9 train cross_entropy <loss>=(0.844487032992)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] #quality_metric: host=algo-1, epoch=1, batch=9 train smooth_l1 <loss>=(0.847855720114)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] Round of batches complete\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] Updated the metrics\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] #quality_metric: host=algo-1, epoch=1, validation mAP <score>=(0.0395081741202)\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:04 INFO 139645234329408] Updating the best model with validation-mAP=0.0395081741202\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:05 INFO 139645234329408] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:05 INFO 139645234329408] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1564956845.070931, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\", \"epoch\": 1}, \"StartTime\": 1564956843.296358}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:05 WARNING 139645234329408] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:05 INFO 139645234329408] Saved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\u001b[0m\n",
      "\u001b[31m[08/04/2019 22:14:05 INFO 139645234329408] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}, \"totaltime\": {\"count\": 1, \"max\": 18749.20082092285, \"sum\": 18749.20082092285, \"min\": 18749.20082092285}, \"setuptime\": {\"count\": 1, \"max\": 11.973857879638672, \"sum\": 11.973857879638672, \"min\": 11.973857879638672}}, \"EndTime\": 1564956845.449255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/Object Detection\"}, \"StartTime\": 1564956826.768138}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-04 22:14:32 Uploading - Uploading generated training model\n",
      "2019-08-04 22:14:32 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 127\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = 's3://{}/{}'.format(sage_bucket, my_bucket+'/train/')\n",
    "s3_validation_data = 's3://{}/{}'.format(sage_bucket, my_bucket+'/validatation/')\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format(sage_bucket, my_bucket)\n",
    "\n",
    "od_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p2.xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "                                         \n",
    "od_model.set_hyperparameters(base_network=network,\n",
    "                             use_pretrained_model=1,\n",
    "                             num_classes=nclass,\n",
    "                             mini_batch_size=mini_batch_size,\n",
    "                             epochs=epochs,\n",
    "                             learning_rate=lr,\n",
    "                             lr_scheduler_step='3,6',\n",
    "                             lr_scheduler_factor=lr_scheduler_factor,\n",
    "                             optimizer=optim,\n",
    "                             momentum=momentum,\n",
    "                             weight_decay=weight_decay,\n",
    "                             overlap_threshold=overlap,\n",
    "                             nms_threshold=nms_thresh,\n",
    "                             image_shape=image_shape,   \n",
    "                             label_width=label_width,\t\t\n",
    "                             num_training_samples=n_train_samples)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "od_model.fit(inputs=data_channels, logs=True)    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049616,
     "end_time": "2019-08-04T22:14:48.200714",
     "exception": false,
     "start_time": "2019-08-04T22:14:48.151098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So now you are training!!! This will take a little while. We are only training for a very small number of epochs (2!), so we don't expect to have a really robust model. Potentially many 100s of epochs may be required depeneding on the quality and amount of training data we have. \n",
    "\n",
    "To level set, this model will be CRAPPY. But that is ok. You now have the basic tools required to set up and improve upon your own problem.\n",
    "\n",
    "ðŸ¤” What are the big considerations as a data scientist?\n",
    "\n",
    "ðŸ¤” What could we do to improve our model?\n",
    "\n",
    "ðŸ¤” How could we evaluate the quality of our data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.065665,
     "end_time": "2019-08-04T22:14:48.318954",
     "exception": false,
     "start_time": "2019-08-04T22:14:48.253289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#object_detector = od_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')   \n",
    "\n",
    "#response = object_detector.predict(data)\n",
    "\n",
    "# Tears down the SageMaker endpoint and endpoint configuration\n",
    "#object_detector.delete_endpoint()\n",
    "\n",
    "# Deletes the SageMaker model\n",
    "#object_detector.delete_model()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "papermill": {
   "duration": 308.032945,
   "end_time": "2019-08-04T22:14:50.504922",
   "environment_variables": {},
   "exception": null,
   "input_path": "osm_ml_training_pt2.ipynb",
   "output_path": "osm_ml_training_pt2_out.ipynb",
   "parameters": {
    "my_bucket": "geohack_sbs",
    "role": "AmazonSageMaker-ExecutionRole-20171129T143774",
    "sage_bucket": "sagemaker-us-east-2-771575179338"
   },
   "start_time": "2019-08-04T22:09:42.471977",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}